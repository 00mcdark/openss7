\documentclass[letterpaper,final,notitlepage,twocolumn,10pt,twoside]{article}
\usepackage{ftnright}
\usepackage{makeidx}
\usepackage{pictex}
%\usepackage{psfig}
%\usepackage{graphics}
\usepackage{graphicx}
\usepackage{eepic}
%\usepackage{epsfig}
%\usepackage[dvips]{graphicx,epsfig}
%\usepackage[dvips]{epsfig}
%\usepackage{epsf}
\usepackage{natbib}
\usepackage{placeins}
%\usepackage{placeins}

\setlength{\voffset}{-1.0in}
\setlength{\topmargin}{0.2in}
\setlength{\headheight}{0.2in}
\setlength{\headsep}{0.2in}
\setlength{\topskip}{0.0in}
\setlength{\footskip}{0.4in}
\setlength{\textheight}{9.8in}

\setlength{\hoffset}{-1.0in}
\setlength{\oddsidemargin}{0.6in}
\setlength{\evensidemargin}{0.4in}
\setlength{\textwidth}{7.5in}

\setlength{\marginparwidth}{0.0in}
\setlength{\marginparsep}{0.0in}

\setlength{\columnsep}{0.2in}
\setlength{\columnwidth}{3.65in}
%\setlength{\columnseprule}{0.25pt}

\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}

\let\Huge\huge
\let\huge\LARGE
\let\LARGE\Large
\let\Large\large
\let\large\normalsize
\let\normalsize\small
\let\small\footnotesize
\let\footnotesize\scriptsize
\let\scriptsize\tiny

\makeatletter
\renewcommand\section{\@startsection {section}{1}{\z@}%
                                   {-2ex \@plus -1ex \@minus -.2ex}%
                                   {1ex \@plus .2ex}%
                                   {\normalfont\large\bfseries}}
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}%
                                     {-1.5ex \@plus -.5ex \@minus -.2ex}%
                                     {1ex \@plus .2ex}%
                                     {\normalfont\normalsize\bfseries}}
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
                                     {-1.25ex\@plus -.5ex \@minus -.2ex}%
                                     {1ex \@plus .2ex}%
                                     {\normalfont\normalsize\bfseries}}
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
                                    {1.5ex \@plus .5ex \@minus .2ex}%
                                    {-1em}%
                                    {\normalfont\normalsize\bfseries\slshape}}
\renewcommand\subparagraph{\@startsection{subparagraph}{5}{\parindent}%
                                       {0ex \@plus 0ex \@minus 0ex}%
                                       {-1em}%
                                      {\normalfont\normalsize\bfseries\slshape}}
\makeatother

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{6}

\pagestyle{plain}
%\pagestyle{myheadings}
%\markboth{B. Bidulock}{B. Bidulock}

\makeglossary

\newcommand{\topfigrule}{\vspace{0.5ex}\rule{\columnwidth}{0.4pt}\vspace{0.5ex} }
\newcommand{\botfigrule}{\vspace{0.5ex}\rule{\columnwidth}{0.4pt}\vspace{0.5ex} }
\newcommand{\dblfigrule}{\vspace{0.5ex}\rule{\textwidth}{0.4pt}\vspace{0.5ex} }

%\bibliographystyle{unsrtnat}
%\bibliographystyle{plainnat}
%\bibliographystyle{ieeetr}
%\bibliographystyle{abbrvnat}
%\bibliographystyle{acm}
%\bibliographystyle{plainnat}
\bibliographystyle{alpha}

\begin{document}

%\begin{titlepage}
%\begin{center}
%    STREAMS vs. Sockets Performance Comparison\\
%    Experimental Test Results
%\end{center}
%\end{titlepage}

\title{STREAMS versus Sockets Performance Comparison for User Datagram Protocol\\[0.5ex]
	{\large \textsl{Experimental Test Results for Linux}}}
\author{Brian F. G. Bidulock\thanks{bidulock@openss7.org}\\
	OpenSS7 Corporation}
\maketitle

\begin{abstract}
\addcontentsline{toc}{section}{Abstract}
With the objective of contrasting performance between STREAMS and legacy
approaches to system facilities, a comparison is made between the tested
performance of the \textsl{Linux NET4} UDP implementation and STREAMS UDP
implementations using the \textsl{Linux Fast-STREAMS}
package.\footnote{\textit{http://www.openss7.org/download.html}}
\end{abstract}

%\tableofcontents

\section[Background]{Background}

\subsection[STREAMS]{STREAMS}
STREAMS is a facility first presented in a paper by Dennis M. Ritchie in 1984
\cite[]{Ritchie84}, originally implemented on 4.1BSD and later part of
\textsl{Bell Laboratories Eighth Edition UNIX}, incorporated into \textsl{UNIX
System V Release 3.0} and enhanced in \textsl{UNIX Sysvem V Release 4} and
further in \textsl{UNIX System V Release 4.2}.  STREAMS was used in SVR4 for
terminal intput/output, pseudo-terminals, pipes, named pipes (FIFOs),
interprocess communication and networking.  Since it release in \textsl{System
V Release 4}, STREAMS has been implemented across a wide range of UNIX,
UNIX-like and UNIX-based systems, making its implementation and use an ipso
facto standard.

STREAMS is a facility that allows for a reconfigurable full duplex
communications path, \textit{Stream}, between a user process and a driver in
the kernel.   Kernel protocol modules can be pushed onto and popped from the
\textit{Stream} between the user process and driver.  The \textit{Stream} can
be reconfigured in this way be a user process.  The user process, neighbouring
protocol modules and the driver communicate with each other using a message
passing scheme closely related to "MOM (Message Oriented Middleware)".  This
permits a loose coupling between protocol modules, drivers and user processes,
allowing a third-party and loadable kernel module approach to be taken toward
the provisioning of protocol modules on platforms supporting STREAMS.

On \textsl{UNIX System V Release 4.2}, STREAMS was used for terminal
input-output, pipes, FIFOs (named pipes), and network communications.  Modern
UNIX, UNIX-like and UNIX-based systems providing STREAMS normally support some
degree of network communications using STREAMS; however, many do not support
STREAMS-based pipe and FIFOs\footnote{For example, AIX.} or terminal
input-output.\footnote{For example, HP-UX.}

\textsl{UNIX System V Release 4.2} supported four Application Programmer
Interfaces (APIs) for accessing the network communications facilities of the
kernel:

\begin{enumerate}

\item {\it Transport Layer Interface (TLI).}

\textsl{TLI} is an acronym for the \textit{Transport Layer Interface}
\cite[]{tli}.  The \textsl{TLI} was the non-standard interface provided by
SVR4, later standardized by \textit{X/Open} as the \textsl{XTI} described
below.  This interface is now deprecated.

\item {\it X/Open Transport Interface (XTI).}

\textsl{XTI} is an acronym for the \textsl{X/Open Transport Interface}
\cite[]{xti}.  The \textsl{X/Open Transport Interface} is a standardization of
the \textsl{UNIX System V Release 4}, \textsl{Transport Layer Interface}.  The
interface consists of an Application Programming Interface implemented as a
shared object library.  The shared object library communicates with a
transport provider Stream using a service primitive interface called the
\textsl{Transport Provider Interface}.

While \textsl{XTI} was implemented directly over STREAMS devices supporting
the \textit{Transport Provider Interface (TPI)} \cite[]{tpi} under SVR4,
several non-traditional approaches exist in implementation:

\item {\it Berkeley Sockets.}

Sockets uses the BSD interface that as developed by BBN for TCP/IP protocol
suite under DND contract and released in BSD 4.2.  BSD Sockets provides a set
of primary API functions that are typically implemented as system calls.  The
BSD Sockets interface is non-standard and is now deprecated.

\item {\it POSIX Sockets.}

Sockets were standardized by the \textit{OpenGroup}\footnote{\it
http://www.opengroup.org/} and \textit{IEEE} in the POSIX standardization
process.  They appear in XNS 5.2 \cite[]{xns}, SUSv1 \cite[]{susv1}, SUSv2
\cite[]{susv2} and SUSv3 \cite[]{susv3}.

\end{enumerate}

On systems traditionally supporting Sockets and then retrofitted to support
STREAMS, there is one approach toward supporting \textsl{XTI} without
refitting the entire networking stack:\footnote{This approach is taken by
True64 (Digital) UNIX.}

\begin{enumerate}
\item {\it XTI over Sockets.}
\end{enumerate}

Typically there are two approaches to implementing XTI on systems not
supporting STREAMS:

\begin{enumerate}

\item {\it XTI over Sockets.}

Several implementations of STREAMS on UNIX utilize the concept of \textsl{XTI}
over Sockets.  This a purely a shared object library approach to providing
\textsl{XTI}.  Under this approach it is possible to use the \textsl{XTI}
application programming interface, but it is not possible to utilize any of
the STREAMS capabilities of an underlying \textit{Transport Provider Interface
(TPI)} stream.

\item {\it TPI over Sockets.}

Several implementations of STREAMS on UNIX utilize the concept of \textsl{TPI}
over Sockets.  Following this approach, a STREAMS pseudo-device driver is
provided that hooks directly into internal socket system calls to implement
the driver, and yet the networking stack remains fundamentally BSD in style.
An alternate approach, taken by the \textsl{Linux iBCS} package was to provide
a pseudo-transport provider using a legacy character device to present the
appearance of a STREAMS transport provider.

\end{enumerate}

Conversely, on systems supporting STREAMS, but not traditionally supporting
Sockets (such as SVR4), there are two approaches toward supporting BSD and
POSIX Sockets based on STREAMS:

\begin{enumerate}

\item {\it Library and cooperating \sl STREAMS module.}

Under this approach, a cooperating module, normally called \texttt{sockmod},
is pushed on a Transport Provider Interface (TPI) Stream.  The library,
normally called \texttt{socklib} or simply \texttt{socket}, and cooperating
\texttt{sockmod} module provide the BBN or POSIX Socket API.  \cite[]{impbsd}
\cite[]{socklib}

\item {\it Library and System Calls.}

Under this approach, the BSD or POSIX Sockets API is implemented as system
calls with the sole exception of the \textbf{\texttt{socket}}(3) call.  The
underlying transport provider is still an \textsl{TPI}-based STREAMS transport
provider, it is just that system calls instead of library calls are used to
implement the interface.  \cite[]{socklib}

\end{enumerate}

\subsubsection[Standardization]{Standardization.}

During the POSIX standardization process, networking and Sockets interfaces
were given special treatment to ensure that both the legacy Sockets approach
and the STREAMS approach to networking were compatible. POSIX has standardized
both the XTI and Sockets programmatic interface to networking.  STREAMS
networking base been POSIX compliant for many years, BSD Sockets, POSIX
Sockets, TLI and XTI interfaces, and were compliant in the \textsl{SVR4.2}
release.  The STREAMS networking provided by \textsl{Linux Fast-STREAMS}
package provides POSIX compliant networking.

Therefore, any application utilizing a Socket or Stream in a POSIX compliant
manner will also be compatible with STREAMS networking.\footnote{This
compatibility is exemplified by the \texttt{netperf} program which does not
distinguish between BSD or STREAMS based networking in their implementation or
use.}

\section[Objective]{Objective}

The question has been asked whether there are performance differences between
a purely BSD-style approach and a STREAMS approach to TCP/IP networking, cf.
\cite[]{demux}.  However, there did not exist a system which permitted both
approaches to be tested on the same operating system.  \textsl{Linux
Fast-STREAMS} running on the GNU/Linux operating system now permits this
comparison to be made.  The objective of the current study, therefore, was to
determine whether, for the Linux operating system, the a STREAMS-based pipe
approach to TCP/IP networking is a viable for a replacement for the BSD-style
sockets approach provided by Linux, termed NET4.

When developing STREAMS, the authors oft times found that there were a number
of unsupported preconceptions espoused by Linux advocates about both STREAMS
and STREAMS-based networking, as follows:

\begin{itemize}

\item STREAMS is slow.

\item STREAMS is more flexible, but less efficient.\footnote{For example, see
\textit{http://www.kernel.org/pub/linux/docs/lkml/\#s9-9} .}

\item STREAMS performs poorly on uniprocessor and ever poorer on SMP.

\item STREAMS networking is slow.

\item STREAMS networking is unnecessarily complex and cumbersome.

\end{itemize}

The current study attempts to determine the validity of these preconceptions.

%% For example, the Linux kernel mailing list has this to say about STREAMS:
%% 
%% \begin{quote}
%% \begin{description}
%% \item[(REG)] STREAMS allow you to "push" filters onto a network stack.
%% The idea is that you can have a very primitive network stream of data, and
%% then "push" a filter ("module") that implements TCP/IP or whatever on top of
%% that.  Conceptually, this is very nice, as it allows clean separation of your
%% protocol layers.  Unfortunately, implementing STREAMS poses many performance
%% problems.  Some Unix STREAMS based server telnet implementations even ran the
%% data up to user space and back down again to a pseudo-tty driver, which is
%% very inefficient.
%% 
%% STREAMS will \textbf{never} be available in the standard Linux kernel, it will
%% remain a separate implementation with some add-on kernel support (that come
%% with the STREAMS package).  Linux and his networking gurus are unanimous in
%% their decision to keep STREAMS out of the kernel.  They have stated several
%% times on the kernel list when this topic comes up that even optional support
%% will not be included.
%% 
%% \item[(REW, quoting Larry McVoy)] "It's too bad, I can see why some
%% people think they are cool, but the performance cost - both on uniprocessors
%% and even more so on SMP boxes - is way too high for STREAMS to ever get added
%% to the Linux kernel."
%% 
%% Please stop asking for them, we have agreement amoungst the head guy, the
%% networking guys, and the fringe folks like myself that they aren't going in.
%% 
%% \item[(REG, quoting Dave Grothe, the STREAMS guy)] STREAMS is a good
%% framework for implementing complex and/or deep protocol stacks having nothing
%% to do with TCP/IP, such as SNA.  It trades some efficiency for flexibility.
%% You may find the Linux STREAMS package (LiS) to be quite useful if you need to
%% port protocol drivers from Solaris or UnixWare, as Caldera did.
%% \end{description}
%% 
%% The Linux STREAMS (LiS) package is available for download if you want to use
%% STREAMS for Linux.  The following site also contains a dissenting view, which
%% supports STREAMS.
%% \end{quote}

\section[Description]{Description}

The three implementations tested vary in their implementation details.  These
implementation details are described below.

\subsection[Linux NET4 Sockets]{Linux NET4 Sockets}

Normally, in BSD-style implementations of Sockets, Sockets is not merely the
Application Programmer Interface, but also consists of a more general purpose
network protocol stack implementation \cite[]{bsd}, even though the mechanism
is not used for more than TCP/IP networking.  \cite[]{magic}

Although BSD networking implementations consist of a number of networking
layers with soft interrupts used for each layer of the networking stack
\cite[]{bsd}, the Linux implementation, although based on the the BSD
approach, collapses the socket, protocol and device levels of the BSD stack
into a single layer.

The Linux UDP (User Datagram Protocol) implementation is a good example of
this collapse.

\subparagraph*{Write side processing.} On the write side of the Socket, bytes
are copied from the user into allocated socket buffers.  Write side socket
buffers are charged against the send buffer.  Socket buffers are immediately
dispatched to the IP layer for processing.  When the IP layer (or a driver)
consumed the socket buffer, it releases the amount of send buffer space that
was charged for the send buffer.  If there is insufficient space in the send
buffer to accommodate the write, the calling processed is either blocked or
the system call returns an error.

For loop-back operation, immediately sending the socket buffer to the IP layer
has the additional ramification that the socket buffer is immediately struck
from the send buffer and immediately added to the receive buffer on the
receiving socket.  Therefore, the size of the send buffer or the send low
water mark, have no effect.

Primarily where Linux differs from BSD and STREAMS approaches is in
dispatching socket buffers to the protocol and the network layers.  Under BSD,
the \texttt{mbuf} is queued against the UDP protocol layer and the UDP layer
software interrupt is raised \cite[]{bsd}.  When the buffer is later
processed, it is queued against the IP protocol layer and the IP layer
software interrupt is raised.  When the buffer is later processed, it is
delivered to the driver.

Linux, on the other hand, passes the socket buffer to the device driver in
user context without intermediate queueing.

Because of this fundamental difference, it is expected that comparisons
between the Linux Socket implementation and STREAMS will differ radically from
any such comparison between a true \textsl{BSD} Socket implementation and
STREAMS.

\subparagraph*{Read side processing.} On the read side of the Socket, the
network layer calls the protocol's receive function.  The receive function
checks if socket is locked (by a reading or writing user).  If the socket is
locked the socket buffer placed in the socket's backlog queue.  The backlog
queue can hold a maximum number of socket buffers.  If this maximum is
exceeded, the packet is dropped.  If the socket is unlocked, and the socket
buffer will fit in the socket's receive buffer, the socket buffer is charged
against the receive buffer.  If the socket buffer will not fit in the receive
buffer, the socket buffer is dropped.

Read side processing under Linux does not drastically differ from BSD as it
does on the write side, except for loop-back devices.  Normally, for
non-loop-back devices, \texttt{skbuff}s received by the device are queued
against the IP layer and the IP layer software interrupt is raised.  When the
software interrupt runs, \texttt{skbuffs}s are delivered directly to the
transport protocol layer without intermediate queueing.

For loop-back operation, however, Linux skips queueing at the IP protocol
layer (which does not exist as it does in BSD) and, instead, directly delivers
\texttt{skbuff}s directly to the transport protocol.

Because of this fundamental difference between Linux and \textsl{BSD} on the
read side, it is expected that performance results for Linux would vary
drastically from that of \textsl{BSD}, and the results of this testing would
therefore not be applicable to \textsl{BSD}.

\subparagraph*{Buffering.} Buffering at the Socket consist of a send buffer
and low water mark and a receive buffer and low water mark.  When the send
buffer is consumed with outstanding messages, writing processes with either
block or the system call will fail with an error.   When the send buffer is
full higher than the low water mark, a blocked writing process will not be
awoken (regardless of whether the process is blocked in write or blocked in
poll/select).  The send low water mark for Linux is typically set to one-half
of the send buffer.

It should be noted that for loop-back operation under Linux, the send
buffering mechanism is effectively defeated.

When the receive buffer is consumed with outstanding messages, received
messages will be discarded.  This is in rather stark contrast to BSD where
messages are effectively returned to the network layer when the socket receive
buffer is full and the network layer can determine whether messages should be
discarded or queued further.

When there is no data in the receive buffer, the reading process with either
block or return from the system call with an error.  When the receive buffer
has fewer bytes of data in it than the low water mark, a blocked reading
process will not be awoken (regardless of whether the process is blocked in
write or blocked in poll/select).  The receive low water mark for Linux is
typically set to 1 byte.\footnote{The fact that Linux sets the receive low
water mark to 1 byte is an indication that the buffering mechanism on the read
side simply does not work.}

It should be noted that the Linux buffering mechanism does not have hysteresis
like that of STREAMS.  When the amount of data in the send buffer exceeds the
low water mark, poll will cease to return \texttt{POLLOUT}; when the receive
buffer is less than the low water mark, poll will cease to return
\texttt{POLLIN}.

\subparagraph*{Scheduling.} Scheduling of processes and the buffering
mechanism are closely related.

Writing processes for loop-back operation under UDP are allowed to spin
wildly.  Written data charged against the send buffer is immediately released
when the loop-back interface is encountered and immediately delivered to the
receiving socket (or discarded).  If the writing process is writing data
faster that the reading process is consuming it, the excess will simply be
discarded, and no back-pressure signalled to the sending socket.

If receive buffer sizes are sufficiently large, the writing process will lose
the processor on uniprocessor systems and the reading process scheduled before
the buffer overflows.  If they are not, the excess will be discarded.  On
multiprocessor systems, provided that the read operation takes less time than
the write operation, the reading process will be able to keep pace with the
writing process.  If the receiving process is run with a very low priority,
the writing process will always have the processor and a large percentage of
the written messages will be discarded.

It should be noted that this is likely a Linux-specific deficiency as the BSD
system introduces queueing, even on loop-back.

Reading processes for loop-back operation under UDP are awoken whenever a
single byte is received (due to the receive low water mark).  If the reading
process has higher priority than the writing process on uniprocessors, the
reading process will be awoken for each message sent and the reading process
will read that message before the writing process is permitted to write
another.  On SMP systems, because reading processes will likely have the
socket locked while reading each message, backlog processing will likely be
invoked.

\subsection[Linux Fast-STREAMS]{Linux Fast-STREAMS}

\textsl{Linux Fast-STREAMS} is an implementation of \textsl{SVR4.2 STREAMS}
for the \textsl{GNU/Linux} system developed by the \textsl{OpenSS7
Project}\footnote{\it http://www.openss7.org/} as a replacement for the
buggy, under-performing and now deprecated \textsl{Linux STREAMS (LiS)}
package.

\textsl{Linux Fast-STREAMS} provides the STREAMS executive and interprocess
communication facilities (pipes and FIFOs).  Add-on packages provide
compatibility between \textsl{Linux Fast-STREAMS} and other STREAMS
implementations, a complete \textsl{XTI} shared object library, and transport
providers.  Transport providers for the TCP/IP suite consist of an
\texttt{inet} driver that uses the \textit{XTI over Sockets} approach as well
as a full STREAMS implementation of SCTP (Stream Control Transmission
Protocol), UDP (User Datagram Protocol) and RAWIP (Raw Internet Protocol).

\subsubsection[XTI over Sockets]{XTI over Sockets} The XTI over Sockets
implementation is the \texttt{inet} STREAMS driver developed by the
\textsl{OpenSS7 Project}.\footnote{\textit{http://www.openss7.org/}}  This
driver is implemented as a STREAMS pseudo-device driver and uses STREAMS for
passing TPI service primitives to and from upstream modules or the
\textit{Stream head}.  Within the driver, data and other TPI service
primitives are translated into kernel socket calls to an socket that was
opened by the driver corresponding to the transport provider instance.  Events
received from this internal socket are also translated into transport provider
service primitives and passed upstream.

\subparagraph*{Write side processing.} Write side processing uses standard
STREAMS flow control mechanisms, as are described for TPI, below, with the
exception that once the message blocks arrive at the driver, they are passed
to the internal socket.

Therefore, a unique characteristic of the write side processing for the XTI
over Sockets driver is that data is first copied from user space into STREAMS
message blocks and then copied again from the STREAMS message blocks to the
socket.  This constitutes two copies per byte versus one copy per byte and has
a significant impact on the performance of the driver at large message
sizes.\footnote{This expectation of peformance impact is held up by the test
results.}

\subparagraph*{Read side processing.} Read side processing uses standard
STREAMS flow control mechanisms as are described for TPI, below.

A unique characteristic of the read side processing fro the XTI over Sockets
driver is that data is first copied from the internal socket to a STREAMS
message block and then copied again from the STREAMS message block to user
space.  This constitutes two copies per byte versus one copy per byte and has
a significant impact on the performance of the driver at large message
sizes.\footnote{This expectation of peformance impact is held up by the test
results.}

\subparagraph*{Buffering.} Buffering uses standard STREAMS queueing and flow
control mechanisms as are described for TPI, below.

\subparagraph*{Scheduling.} Scheduling resulting from queueing and flow
control are the same as described for TPI below.  Considering that the
internal socket used by the driver is on the loop-back interface, data written
on the sending socket appears immediately at the receiving socket or is
discarded.

\subsubsection[TPI]{TPI} The TPI implementation of UDP is a direct STREAMS
implementation that uses the \texttt{udp} driver developed by the
\textsl{OpenSS7 Project}.\footnote{\textit{http://www.openss7.org/}} This
driver interfaces to Linux at the network layer, but provides a complete
STREAMS implementation of the transport layer.  Interfacing with Linux at the
network layer provides for de-multiplexed STREAMS architecture \cite[]{demux}.
The driver presents the Transport Provider Interface (TPI) \cite[]{tpi} for use
by upper level modules and the XTI library \cite[]{xti}.

\subparagraph*{Write side processing.} Write side processing follows standard
STREAMS flow control.  When a write occurs at the \textit{Stream head}, the
\textit{Stream head} checks for downstream flow control on the write queue.
If the \textit{Stream} is flow controlled, the calling process is blocked or
the write system call fails (\texttt{EAGAIN}).  When the \textit{Stream} is
not flow controlled, user data is transferred to allocated message blocks and
passed downstream.  When the message blocks arrive at a downstream queue, the
count of the data in the message blocks is added to to the queue count.  If
the queue count exceeds the high water mark defined for the queue, the queue
is marked full and subsequent upstream flow control tests will fail.

\subparagraph*{Read side processing.} Read side processing follows standard
STREAMS flow control.  When a read occurs at the \textit{Stream head}, the
\textit{Stream head} checks the read queue for messages.  If the read queue
has no messages queued, the queue is marked to be enabled when messages arrive
and the calling process is either blocked or the system call returns an error
(\texttt{EAGAIN}).  If messages exist on the read queue, they are dequeued and
data copied from the message blocks to the user supplied buffer.  If the
message block is completely consumed, it is freed; otherwise, the message
block is placed back on the read queue with the remaining data.

\subparagraph*{Buffering.} Buffering follows the standard STREAMS queueing and
flow control mechanisms.

When a queue is found empty by a reading process, the fact that the queue
requires service is recorded.  Once the first message arrives at the queue
following a process finding the queue empty, the queue's service procedure
will be scheduled with the STREAMS scheduler.

When a queue is tested for flow control and the queue is found to be full, the
fact that a process wishes to write the to queue is recorded.  When the count
of the data on the queue falls beneath the low water mark, previous queues
will be back enabled (that is, their service procedures will be scheduled with
the STREAMS scheduler).

\subparagraph*{Scheduling.}

\section[Method]{Method}

\paragraph*{Test Facilities.}

To test the performance of STREAMS-based pipes, the \textsl{Linux
Fast-STREAMS} package was
used.\footnote{\textit{http://www.openss7.org/download.html}} The
\textsl{Linux Fast-STREAMS} package builds and install Linux loadable kernel
modules and includes the \texttt{netperf} program used for testing.

\paragraph*{Test Program.}

The program used is a version of the \texttt{netperf} network performance
measurement tool developed and maintained by Rick Jones for
\textit{Hewlett-Packard}.  This modified version is available from the
\textit{OpenSS7 Project}.\footnote{http://www.openss7.org/download.html}
While the program is able test using both POSIX Sockets and XTI STREAMS
interfaces, modifications were required to the package to allow it to compile
for \textsl{Linux Fast-STREAMS}.

The \texttt{netperf} program has many options.  Therefore, a benchmark script
(called \texttt{netperf\_benchmark}) was used to obtain repeatable raw data
for the various machines and distributions tested.  This benchmark script is
included in the \texttt{netperf} distribution available from the
\textit{OpenSS7 Project}.  A listing of this script is provided in
\textit{Appendix \ref{section:script} (p. \pageref{section:script})}.

\subsection[Distributions Tested]{Distrbutions Tested} To remove the
dependence of test results on a particular Linux kernel or machine, various
Linux distributions were used for testing.  The distributions tested are as
follows:

\begin{tabular}{ll}\\
Distribution & Kernel\\
\hline
RedHat 7.2 & 2.4.20-28.7\\
WhiteBox 3 & 2.4.27\\
CentOS 4 & 2.6.9-5.0.3.EL\\
SuSE 10 OSS & 2.6.13-15-default\\
Ubuntu 6.06 & 2.6.17-11-generic\\
Fedora Core 6 & 2.6.20-1.2933.fc6\\
\end{tabular}\\

\subsection[Test Machines]{Test Machines} To remove the dependence of test
results on a particular machine, various machines were used for testing as
follows:

\begin{tabular}{llll}\\
Hostname & Processor & Memory & Architecture\\
\hline
porky & 2.57GHz PIV & 1Gb (333MHz) & i686 UP\\
pumbah & 2.57GHz PIV & 1Gb (333MHz) & i686 UP\\
daisy & 3.0GHz i630 HT & 1Gb (400MHz) & x86\_64 SMP\\
mspiggy & 1.7GHz PIV & 1Gb (333MHz) & i686 UP\\
\end{tabular}\\

\section[Results]{Results}

\subsection[Porky]{Porky} Porky is a 2.57GHz Pentium IV (i686) uniprocessor
machine with 1Gb of memory.  Linux distributions tested on this machine are as
follows:

\begin{tabular}{ll}\\
Distribution & Kernel\\
\hline
Fedora Core 6 & 2.6.20-1.2933.fc6\\
CentOS 4 & 2.6.9-5.0.3.EL\\
SuSE 10 OSS & 2.6.13-15-default\\
Ubuntu 6.06 & 2.6.17-11-generic\\
\end{tabular}

\subsubsection[Fedora Core 6]{Fedora Core 6} Fedora Core 6 is the most recent
full release Fedora distribution.  This distribution sports a 2.6.20-1.2933.fc6 kernel
with the latest patches.  This is the \texttt{x86} distribution with recent
updates.

\subparagraph*{Performance.} \textit{Figure \ref{figure:fc6perf} (p.
\pageref{figure:fc6perf})} illustrates the performance of Sockets compared to
XTI over Socket and XTI approaches.  STREAMS demonstrates significant
improvements at message sizes of less than 1024 bytes.

\subparagraph*{Delay.} \textit{Figure \ref{figure:fc6dly} (p.
\pageref{figure:fc6dly})} illustrates the message delay of Sockets compared to
XTI over Socket and XTI approaches.  STREAMS demonstrates significant
improvements at message sizes of less than 1024 bytes.

From the figure, it can be seen that the slope of the delay graph for STREAMS
and Sockets are about the same.  This is expected as both implementations use
the same function to copy message bytes to and from user space.  The slope of
the XTI over Sockets graph is over twice the slope of the Sockets graph which
reflects the fact that XTI over Sockets performs multiple copies of the data:
two copies on the send side and two copies on the receive side.

The intercept for STREAMS is lower than Sockets, indicating that STREAMS has a
lower per-message overhead than Sockets, despite the fact that the destination
address is being copied to and from user space for each message.

\subparagraph*{Throughput.} \textit{Figure \ref{figure:fc6thrput} (p.
\pageref{figure:fc6thrput})} illustrates the throughput of Sockets compared to
XTI over Socket and XTI approaches.  STREAMS demonstrates significant
improvements at all message sizes.

As can be seen from the figure, all implementations exhibit strong power
function characteristics (at least at lower write sizes), indicating structure
and robustness for each implementation.  The slight concave downward curvature
of the graphs at large message sizes indicates some degree of saturation.

\subparagraph*{Improvement.} \textit{Figure \ref{figure:fc6comp} (p.
\pageref{figure:fc6comp})} illustrates the comparison of Sockets to XTI over
Socket and XTI approaches.  STREAMS demonstrates significant improvements
(approx. 30\% improvement) at message sizes below 1024 btyes.
Perhaps surprising is that the XTI over Sockets approach rivals (95\%) Sockets
alone at small message sizes (where multiple copies are not controlling).

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_fc6_perf}
\caption[FC6 on Porky Performance]{FC6 on Porky Performance}
\label{figure:fc6perf}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_fc6_delay}
\caption[FC6 on Porky Delay]{FC6 on Porky Delay}
\label{figure:fc6dly}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_fc6_thrput}
\caption[FC6 on Porky Throughput]{FC6 on Porky Throughput}
\label{figure:fc6thrput}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_fc6_comp}
\caption[FC6 on Porky Comparison]{FC6 on Porky Comparison}
\label{figure:fc6comp}
\end{figure}

\subsubsection[CentOS 4.0]{CentOS 4.0}
CentOS 4.0 is a clone of the RedHat Enterprise 4 distribution.  This is the
\texttt{x86} version of the distribution.  The distribution sports a
2.6.9-5.0.3.EL kernel.

\subparagraph*{Performance.} \textit{Figure \ref{figure:centosperf} (p.
\pageref{figure:centosperf})} illustrates the performance of Sockets compared
to XTI over Socket and XTI approaches.  STREAMS demonstrates significant
improvements at message sizes of less than 1024 bytes.

As can be seen from the figure, \textsl{Linux Fast-STREAMS} outperforms Linux
at all message sizes.  Also, and perhaps surprisingly, the XTI over Sockets
implementation also performs as well as Linux at lower message sizes.

\subparagraph*{Delay.} \textit{Figure \ref{figure:centosdly} (p.
\pageref{figure:centosdly})} illustrates the message delay of Sockets compared
to XTI over Socket and XTI approaches.  STREAMS demonstrates significant
improvements at message sizes of less than 1024 bytes.

Both STREAMS and Sockets exhibit the same slope, and XTI over Sockets exhibits
over twice the slope, indicating that copies of data control the per-byte
characteristics.  STREAMS exhibits a lower intercept than both other
implementations, indicating that STREAMS has the lowest per-message overhead,
regardless of copying the destination address to and from the user with each
sent and received message.

\subparagraph*{Throughput.} \textit{Figure \ref{figure:centosthrput} (p.
\pageref{figure:centosthrput})} illustrates the throughput of Sockets compared
to XTI over Socket and XTI approaches.  STREAMS demonstrates significant
improvements at all message sizes.

As can be seen from the figure, all implementations exhibit strong power
function characteristics (at least at lower write sizes), indicating structure
and robustness for each implementation.  Again, the sligh concave downward
curvature at large memory sizes indicates memory bus saturation.

\subparagraph*{Improvement.} \textit{Figure \ref{figure:centoscomp} (p.
\pageref{figure:centoscomp})} illustrates the comparison of Sockets to XTI
over Socket and XTI approaches.  STREAMS demonstrates significant improvements
(approx. 30-40\% improvement) at message sizes below 1024 bytes.
Perhaps surprising is that the XTI over Sockets approach rivals (97\%) Sockets
alone at small message sizes (where multiple copies are not controlling).

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_centos_perf}
\caption[CentOS on Porky Performance]{CentOS on Porky Performance}
\label{figure:centosperf}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_centos_delay}
\caption[CentOS on Porky Delay]{CentOS on Porky Delay}
\label{figure:centosdly}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_centos_thrput}
\caption[CentOS on Porky Throughput]{CentOS on Porky Throughput}
\label{figure:centosthrput}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_centos_comp}
\caption[CentOS on Porky Comparison]{CentOS on Porky Comparison}
\label{figure:centoscomp}
\end{figure}

\subsubsection[SuSE 10 OSS]{SuSE 10 OSS}
SuSE 10 OSS is the public release version of the SuSE/Novell distribution.
There have been two releases subsequent to this one: the 10.1 and recent 10.2
releases.  The SuSE 10 release sports a 2.6.13 kernel and the
2.6.13-15-default kernel was the tested kernel.

\subparagraph*{Performance.}
\textit{Figure \ref{figure:suseperf} (p. \pageref{figure:suseperf})}
illustrates the performance of Sockets compared to XTI over Socket and XTI
approaches.  STREAMS demonstrates significant improvements at all message
sizes.

\subparagraph*{Delay.}
\textit{Figure \ref{figure:susedly} (p. \pageref{figure:susedly})} illustrates
the message delay of Sockets compared to XTI over Socket and XTI approaches.
STREAMS demonstrates significant improvements at all message sizes.

Again, STREAMS and Sockets exhibit the same slope, and XTI over Sockets more
than twice the slope.  STREAMS again has a significantly lower intercept and
the XTI over Sockets and Sockets intercepts are similar, indicating that
STREAMS has a smaller per-message overhead, despite copying destination
addresses with each message.

\subparagraph*{Throughput.}
\textit{Figure \ref{figure:susethrput} (p. \pageref{figure:susethrput})}
illustrates the throughput of Sockets compared to XTI over Socket and XTI
approaches.  STREAMS demonstrates significant improvements at all message
sizes.

As can be seen from \textit{Figure \ref{figure:susethrput}}, all
implementations exhibit strong power function characteristics (at least at
lower write sizes), indicating structure and robustness for each
implementation.

\subparagraph*{Improvement.}
\textit{Figure \ref{figure:susecomp} (p. \pageref{figure:susecomp})}
illustrates the comparison of Sockets to XTI over Socket and XTI approaches.
STREAMS demonstrates significant improvements (25-30\%) at all message sizes.

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_suse_perf}
\caption[SuSE on Porky Performance]{SuSE on Porky Performance}
\label{figure:suseperf}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_suse_delay}
\caption[SuSE on Porky Delay]{SuSE on Porky Delay}
\label{figure:susedly}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_suse_thrput}
\caption[SuSE on Porky Throughput]{SuSE on Porky Throughput}
\label{figure:susethrput}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_suse_comp}
\caption[SuSE on Porky Comparison]{SuSE on Porky Comparison}
\label{figure:susecomp}
\end{figure}

\subsubsection[Ubuntu 6.06]{Ubuntu 6.06}
Ubuntu 6.06 is the current release of the Ubuntu distribution.  The Ubuntu
6.06 release sports a 2.6.15 kernel.  The tested distribution had current
updates applied.

\subparagraph*{Performance.}
\textit{Figure \ref{figure:ubuntuperf} (p. \pageref{figure:ubuntuperf})}
illustrates the performance of Sockets compared to XTI over Socket and XTI
approaches.  STREAMS demonstrates marginal improvements (~5\%) at all message
sizes.

\subparagraph*{Delay.}
\textit{Figure \ref{figure:ubuntudly} (p. \pageref{figure:ubuntudly})}
illustrates the message delay of Sockets compared to XTI over Socket and XTI
approaches.  STREAMS demonstrates marginal improvements at all message sizes.

Although STREAMS exhibits the same slope (per-byte processing overhead) as
Sockets, Ubuntu and the 2.6.17 kernel are the only combination where the
STREAMS intercept is not significantly lower than Sockets.  Also, the XTI over
Sockets slope is steeper and the XTI over Sockets intercept is much larger
than Sockets alone.

\subparagraph*{Throughput.}
\textit{Figure \ref{figure:ubuntuthrput} (p.  \pageref{figure:ubuntuthrput})}
illustrates the throughput of Sockets compared to XTI over Socket and XTI
approaches.  STREAMS demonstrates marginal improvements at all message
sizes.

As can be seen from \textit{Figure \ref{figure:ubuntuthrput}}, all
implementations exhibit strong power function characteristics (at least at
lower write sizes), indicating structure and robustness for each
implementation.

\subparagraph*{Improvement.}
\textit{Figure \ref{figure:ubuntucomp} (p. \pageref{figure:ubuntucomp})}
illustrates the comparison of Sockets to XTI over Socket and XTI approaches.
STREAMS demonstrates marginal improvements (~5\%) at all message sizes.

Unbuntu is the only distribution tested where STREAMS does not show
significant improvements over Sockets.  Nevertheless, STREAMS does show
marginal improvement (~5\%) over all message sizes and performed better than
Sockets at all message sizes.

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_ubuntu_perf}
\caption[Ubuntu 6.06 on Porky Performance]{Ubuntu 6.06 on Porky Performance}
\label{figure:ubuntuperf}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_ubuntu_delay}
\caption[Ubuntu 6.06 on Porky Delay]{Ubuntu 6.06 on Porky Delay}
\label{figure:ubuntudly}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_ubuntu_thrput}
\caption[Ubuntu 6.06 on Porky Throughput]{Ubuntu 6.06 on Porky Throughput}
\label{figure:ubuntuthrput}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_ubuntu_comp}
\caption[Ubuntu 6.06 on Porky Comparison]{Ubuntu 6.06 on Porky Comparison}
\label{figure:ubuntucomp}
\end{figure}

\subsection[Pumbah]{Pumbah}
Pumbah is a 2.57GHz Pentium IV (i686) uniprocessor machine with 1Gb of memory.
This machine differs from Porky in memory type only (Pumbah has somewhat
faster memory than Porky.) Linux distributions tested on this machine are as
follows:

\begin{tabular}{ll}\\
Distribution & Kernel\\
\hline
RedHat 7.2 & 2.4.20-28.7\\
\end{tabular}\\[1.0ex]

Pumbah is a control machine and is used to rule out differences between recent
2.6 kernels and one of the oldest and most stable 2.4 kernels.

\subsubsection[RedHat 7.2]{RedHat 7.2}
RedHat 7.2 is one of the oldest (and arguably the most stable) glibc2 based
releases of the RedHat distribution.  This distribution sports a 2.4.20-28.7
kernel.  The distribution has all available updates applied.

\subparagraph*{Performance.}
\textit{Figure \ref{figure:rh7perf} (p. \pageref{figure:rh7perf})} illustrates
the performance of Sockets compared to XTI over Socket and XTI approaches.
STREAMS demonstrates significant improvements at all message sizes, and
staggering improvements at large message sizes.

\subparagraph*{Delay.}
\textit{Figure \ref{figure:rh7dly} (p. \pageref{figure:rh7dly})} illustrates
the message delay of Sockets compared to XTI over Socket and XTI approaches.
STREAMS demonstrates significant improvements at all message sizes, and
staggering improvements at large message sizes.

The slope of the STREAMS delay curve is much lower than (almost half that of)
the Sockets delay curve, indicating that STREAMS is exploiting some memory
efficiencies not possible in the Sockets implementation.

%On Linux 2.6 kernels, STREAMS uses RCU for memory caches for message blocks.
%on Linux 2.4 kernels, RCU is not available, and STREAMS uses its own hot free
%list for message blocks.  This would in part expain the significantly faster
%per-byte performance of STREAMS, if it were not for the Mspiggy control
%machine that exhibits the same characteristic on a 2.6 kernel.

\subparagraph*{Throughput.}
\textit{Figure \ref{figure:rh7thrput} (p. \pageref{figure:rh7thrput})}
illustrates the throughput of Sockets compared to XTI over Socket and XTI
approaches.  STREAMS demonstrates improvements at all message sizes.

As can be seen from \textit{Figure \ref{figure:rh7thrput}}, all
implementations exhibit strong power function characteristics (at least at
lower write sizes), indicating structure and robustness for each
implementation.

The Linux NET4 UDP implementation results deviate more sharply from power
function behaviour at high message sizes.  This also, is rather different that
the 2.6 kernel situation.  One contributing factor is the fact that neither
the send nor receive buffers can be set above 65,536 bytes on this version of
Linux 2.4 kernel.  Tests were performed with send and receive buffer size
requests of 131,072 bytes.  Both the STREAMS XTI over Sockets UDP
implementation and the Linux NET4 UDP implementation suffer from the maximum
buffer size, whereas, the STREAMS UDP implementation implements and permits
the larger buffers.

\subparagraph*{Improvement.}
\textit{Figure \ref{figure:rh7comp} (p. \pageref{figure:rh7comp})} illustrates
the comparison of Sockets to XTI over Socket and XTI approaches.  STREAMS
demonstrates significant improvements all message sizes.

The more dramatic improvements over Linux NET4 UDP and XTI over Sockets UDP is
likely due in part to the restriction on buffer sizes in 2.4 as described
above.

Unfortunately, the RedHat 7.2 system does not appear to have acted as a very
good control system.  The differences in maximum buffer size make any
differences from other tested behaviour obvious.

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_rh7_perf}
\caption[RedHat 7.2 Pumbah Performance]{RedHat 7.2 Pumbah Performance}
\label{figure:rh7perf}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_rh7_delay}
\caption[RedHat 7.2 Pumbah Delay]{RedHat 7.2 Pumbah Delay}
\label{figure:rh7dly}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_rh7_thrput}
\caption[RedHat 7.2 Pumbah Throughput]{RedHat 7.2 Pumbah Throughput}
\label{figure:rh7thrput}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_rh7_comp}
\caption[RedHat 7.2 Pumbah Comparison]{RedHat 7.2 Pumbah Comparison}
\label{figure:rh7comp}
\end{figure}

\subsection[Daisy]{Daisy}
Daisy is a 3.0GHz i630 (x86\_64) hyperthreaded machine with 1Gb of memory.
Linux distributions tested on this machine are as follows:

\begin{tabular}{ll}\\
Distribution & Kernel\\
\hline
Fedora Core 6 & 2.6.20-1.2933.fc6\\
\end{tabular}\\[1.0ex]

This machine is used as an SMP control machine.  Most of the test were
performed on uniprocessor non-hyperthreaded machines.  This machine is
hyperthreaded and runs full SMP kernels.  This machine also supports EMT64 and
runs \texttt{x86\_64} kernels.  It is used to rule out both SMP differences as
well as 64-bit architecture differences.

\subsubsection[Fedora Core 6 (x86\_64)]{Fedora Core 6 (x86\_64)}
Fedora Core 6 is the most recent full release Fedora distribution.  This
distribution sports a 2.6.20-1.2933.fc6 kernel with the latest patches.  This is the
\texttt{x86\_64} distribution with recent updates.

\subparagraph*{Performance.}
\textit{Figure \ref{figure:smpperf} (p. \pageref{figure:smpperf})} illustrates
the performance of Sockets compared to XTI over Socket and XTI approaches.
STREAMS demonstrates significant improvements at message sizes of less than
1024 bytes.

\subparagraph*{Delay.}
\textit{Figure \ref{figure:smpdly} (p. \pageref{figure:smpdly})} illustrates
the message delay of Sockets compared to XTI over Socket and XTI approaches.
STREAMS demonstrates significant improvements at message sizes of less than
1024 bytes.

The slope of the delay curve either indicates that Sockets is using slightly
larger buffers than STREAMS, or that Sockets is somehow exploiting some
per-byte efficiencies at larger message sizes not acheived by STREAMS.
Nevertheless, the STREAMS intercept is so low that the delay curve for STREAMS
is everywhere beneath that of Sockets.

\subparagraph*{Throughput.}
\textit{Figure \ref{figure:smpthrput} (p. \pageref{figure:smpthrput})}
illustrates the throughput of Sockets compared to XTI over Socket and XTI
approaches.  STREAMS demonstrates significant improvements at all message
sizes.

As can be seen from \textit{Figure \ref{figure:smpthrput}}, all
implementations exhibit strong power function characteristics (at least at
lower write sizes), indicating structure and robustness for each
implementation.

\subparagraph*{Improvement.}
\textit{Figure \ref{figure:smpcomp} (p. \pageref{figure:smpcomp})} illustrates
the comparison of Sockets to XTI over Socket and XTI approaches.  STREAMS
demonstrates significant improvements (approx. 40\% improvement) at message
sizes below 1024 bytes.  That STREAMS UDP gives a 40\% improvement over a wide
range of message sizes on SMP is a dramatic improvement.  Statements regarding
STREAMS networking running poorer on SMP than on UP are quite wrong, at least
with regard to \textsl{Linux Fast-STREAMS}.

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_smp_perf}
\caption[Fedora Core 6 (x86\_64) Daisy Performance]{Fedora Core 6 (x86\_64) Daisy Performance}
\label{figure:smpperf}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_smp_delay}
\caption[Fedora Core 6 (x86\_64) Daisy Delay]{Fedora Core 6 (x86\_64) Daisy Delay}
\label{figure:smpdly}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_smp_thrput}
\caption[Fedora Core 6 (x86\_64) Daisy Throughput]{Fedora Core 6 (x86\_64) Daisy Throughput}
\label{figure:smpthrput}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_smp_comp}
\caption[Fedora Core 6 (x86\_64) Daisy Comparison]{Fedora Core 6 (x86\_64) Daisy Comparison}
\label{figure:smpcomp}
\end{figure}

\subsection[Mspiggy]{Mspiggy}
Mspiggy is a 1.7Ghz Pentium IV (M-processor) uniprocessor notebook (Toshiba
Satellite 5100) with 1Gb of memory.  Linux distributions tested on this
machine are as follows:

\begin{tabular}{ll}\\
Distribution & Kernel\\
\hline
SuSE 10 OSS & 2.6.13-15-default\\
\end{tabular}\\[1.0ex]

Note that this is the same distribution that was also tested on Porky.  The
purpose of testing on this notebook is to rule out the differences between
machine architectures on the test results.  Tests performed on this machine
are control tests.

\subsubsection[SuSE 10 OSS]{SuSE 10 OSS}
SuSE 10 OSS is the public release version of the SuSE/Novell distribution.
There have been two releases subsequent to this one: the 10.1 and recent 10.2
releases.  The SuSE 10 release sports a 2.6.13 kernel and the
2.6.13-15-default kernel was the tested kernel.

\subparagraph*{Performance.}
\textit{Figure \ref{figure:nbperf} (p. \pageref{figure:nbperf})} illustrates
the performance of Sockets compared to XTI over Socket and XTI approaches.
STREAMS demonstrates significant improvements at all message sizes, and
staggering improvements at large message sizes.

\subparagraph*{Delay.}
\textit{Figure \ref{figure:nbdly} (p. \pageref{figure:nbdly})} illustrates
the message delay of Sockets compared to XTI over Socket and XTI approaches.
STREAMS demonstrates significant improvements at all message sizes, and
staggering improvements at large message sizes.

The slope of the STREAMS delay curve is much lower than (almost half that of)
the Sockets delay curve, indicating that STREAMS is exploiting some memory
efficiencies not possible in the Sockets implementation.

\subparagraph*{Throughput.}
\textit{Figure \ref{figure:nbthrput} (p. \pageref{figure:nbthrput})}
illustrates the throughput of Sockets compared to XTI over Socket and XTI
approaches.  STREAMS demonstrates improvements at all message sizes.

As can be seen from \textit{Figure \ref{figure:nbthrput}}, all
implementations exhibit strong power function characteristics (at least at
lower write sizes), indicating structure and robustness for each
implementation.

The Linux NET4 UDP implementation results deviate more sharply from power
function behaviour at high message sizes.  One contributing factor is the fact
that neither the send nor receive buffers can be set above about 111,000 bytes
on this version of Linux 2.6 kernel running on this speed of processor.  Tests
were performed with send and receive buffer size requests of 131,072 bytes.
Both the STREAMS XTI over Sockets UDP implementation and the Linux NET4 UDP
implementation suffer from the maximum buffer size, whereas, the STREAMS UDP
implementation implements and permits the larger buffers.

\subparagraph*{Improvement.}
\textit{Figure \ref{figure:nbcomp} (p. \pageref{figure:nbcomp})} illustrates
the comparison of Sockets to XTI over Socket and XTI approaches.  STREAMS
demonstrates significant improvements all message sizes.

The more dramatic improvements over Linux NET4 UDP and XTI over Sockets UDP is
likely due in part to the restriction on buffer sizes in 2.6 on slower
processors as described above.

Unfortunately, this SuSE 10 OSS system does not appear to have acted as a very
good control system.  The differences in maximum buffer size make any
differences from other tested behaviour obvious.

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_nb_perf}
\caption[SuSE 10 OSS Mspiggy Performance]{SuSE 10 OSS Mspiggy Performance}
\label{figure:nbperf}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_nb_delay}
\caption[SuSE 10 OSS Mspiggy Delay]{SuSE 10 OSS Mspiggy Delay}
\label{figure:nbdly}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_nb_thrput}
\caption[SuSE 10 OSS Mspiggy Throughput]{SuSE 10 OSS Mspiggy Throughput}
\label{figure:nbthrput}
\end{figure}

\begin{figure}[p]
\center\includegraphics[width=3.3in]{netperf_nb_comp}
\caption[SuSE 10 OSS Mspiggy Comparison]{SuSE 10 OSS Mspiggy Comparison}
\label{figure:nbcomp}
\end{figure}

\section[Analysis]{Analysis}

\subsection[Discussion]{Discussion}

The test results reveal that the maximum throughput performance, as tested by
the \texttt{netperf} program, of the STREAMS implementation of UDP is superior
to that of the Linux NET4 Sockets implementation of UDP.  In fact, STREAMS
implementation performance at smaller message sizes is significantly (as must
as 30-40\%) greater than that of Linux NET4 UDP.

As the common belief is that STREAMS would exhibit poorer performance, this is
perhaps a startling result to some.

Looking at both implementations, the differences can be described by
implementation similarities and differences:

\subparagraph*{Send processing.}

When Linux NET4 UDP receives a send request, the available send buffer space
is checked.  If the current data would cause the send buffer fill to exceed
the send buffer maximum, either the calling process blocks awaiting available
buffer, or the system call returns with an error (\texttt{ENOBUFS}).  If the
current send request will fit into the send buffer, a socket buffer
(\texttt{skbuff}) is allocated, data is copied from user space to the buffer,
and the socket buffer is dispatched ot the IP layer for transmission.

Linux 2.6 kernels have an amazing amount of special case code that gets
executed for even a simply UDP send operation.  Linux 2.4 kernels are far more
direct.  The result is the same, they are different, however, in the depths to
which the 2.6 kernel must delve before discovering that a send is just a
simple send.  This might explain part of the rather striking differences
between the performance comparison between STREAMS and NET4 on 2.6 and 2.4
kernels.

When the STREAMS Stream head receives a putmsg(2) request, it checks
downstream flow control.  If the Stream is flow controlled downstream, either
the calling process blocks awaiting succession of flow control, or the
putmsg(2) system call returns with an error (\texttt{EAGAIN}).  if the Stream
is not flow controlled on the write side, message blocks are allocated to hold
the control and data portions of the request and the message blocks are passed
downstream to the driver.  When the driver receives an \texttt{M\_DATA} or
\texttt{M\_PROTO} message block from the Stream head, in its put procedure, it
simply queues it to the driver write queue with putq(9).  putq(9) will result
in the enabling of the service procedure for the driver write queue under the
proper circumstances.  When the service procedure runs, messages will be
dequeued from the driver write queue transformed into IP datagrams and send to
the IP layer for transmission on the network interface.

\textsl{Linux Fast-STREAMS} has a feature whereby the driver can request that
the Stream head allocate a Linux socket buffer (\texttt{skbuff}) to hold the
data buffer associated with an allocated message block.  The STREAMS UDP
driver utilizes this feature.  STREAMS also has the feature that a write
offset can be applied to all data block allocated and passed downstream.  The
STREAMS UDP driver uses this capability also.  The write offset set by the
tested driver was a maximum hard header length.

\subparagraph*{Network processing.}

Network processing (that is the bottom end under the transport protocol) for
both implementations is effectively the same, with only minor differences.  In
the STREAMS UDP implementation, no \texttt{sock} structure exists, so issuing
socket buffers to the network layer is performed in a slightly more direct
fashion.

Loop-back processing is identical as this is performed by the Linux NET4 IP
layer in both cases.

For Linux NET4 UDP, when the IP layer frees or orphans the socket buffer, the
amount of data associated with the socket buffer is subtracted from the
current send buffer fill.  If the current buffer fill is less than 1/2 of the
maximum, all processes blocked on write or blocked on poll are are woken.

For STREAMS UDP, when the IP layer frees or orphans the socket buffer, the
amount of data associated with the socket buffer is subtracted from the
current send buffer fill.  If the current send buffer fill is less than the
send buffer low water mark (\texttt{SO\_SNDLOWAT} or \texttt{XTI\_SNDLOWAT}),
and the write queue is blocked on flow control, the write queue is enabled.

One disadvantage that it is expected would slow STREAMS UDP performance is the
fact that on the sending side, a STREAMS buffer is allocated along with a
\texttt{skbuff} and the \texttt{skbuff} is passed to Linux NET4 IP and the
loop-back device.  For Linux NET4 UDP, the same \texttt{skbuff} is reused on
both sides of the interface.  For STREAMS UDP, there is (currently) no
mechanism for passing through the original STREAMS message block and a new
message block must be allocated.  This results in two message block
allocations per \texttt{skbuff}.

\subparagraph*{Receive processing.}

Under Linux NET4 UDP, when a socket buffer is received from the network layer,
a check is performed whether the associated socket is locked by a user process
or not.  If the associated socket is locked, the socket buffer is placed on a
backlog queue awaiting later processing by the user process when it goes to
release the lock.  A maximum number of socket buffers are permitted to be
queued against the backlog queue per socket (approx. 300).

If the socket is not locked, or if the user process is processing a backlog
before releasing the lock, the message is processed: the receive socket buffer
is checked and if the received message would cause the buffer to exceed its
maximum size, the message is discarded and the socket buffer freed.  If the
received message fits into the buffer, its size is added to the current send
buffer fill and the message is queued on the socket receive queue.  If a
process is sleeping on read or in poll, an immediate wakeup is generated.

In the STREAMS UDP implementation on the receive side, again there is no
\texttt{sock} structure, so the socket locking and backlog techniques
performed by UDP at the lower layer do not apply.  When the STREAMS UDP
implementation receives a socket buffer from the network layer, it tests the
receive side of the Stream for flow control and, when not flow controlled,
allocates a STREAMS buffer using esballoc(9) and passes the buffer directly to
the upstream queue using putnext(9).  When flow control is in effect and the
read queue of the driver is not full, a STREAMS message block is still
allocated and placed on the driver read queue.  When the driver read queue is
full, the received socket buffer is freed and the contents discarded.  While
different in mechanism from the socket buffer and backlog approach taken by
Linux NET4 UDP, this bottom end receive mechanism is similar in both
complexity and behaviour.

\subparagraph*{Buffering.}

For Linux NET4, when a send side socket buffer is allocated, the true size of
the socket buffer is added to the current send buffer fill.  After the socket
buffer has been passed to the IP layer, and the IP layer consumes (frees or
orphans) the socket buffer, the true size of the socket buffer is subtracted
from the current send buffer fill.  When the resulting fill is less than 1/2
the send buffer maximum, sending processes blocked on send or poll are woken
up.  When a send will not fit within the maximum send buffer size considering
the size of the transmission and the current send buffer fill, the calling
process blocks or is returned an error (\texttt{ENOBUFS}).  Processes that
are blocked or subsequently block on poll(2) will not be woken up until the
send buffer fill drops beneath 1/2 of the maximum; however, any process that
subsequently attempts to send and has data that will fit in the buffer will be
permitted to proceed.

\subparagraph*{Scheduling.}

\subsection[Result]{Result}


\section[Conclusions]{Conclusions}

These experiments have shown that the \textsl{Linux Fast-STREAMS}
implementation of STREAMS UDP networking outperforms the Linux NET4
implementation by a significant amount.  All of the conventional wisdom with
regard to STREAMS and STREAMS networking is undermined by these test results.

\begin{itemize}
\item STREAMS is not slow.

In fact STREAMS showed superior performance over Sockets in all tests.

\item STREAMS is more flexixble {\em and} more efficient.

As STREAMS showed superior performance over Sockets in all tests, it is both
more flexible and more efficient.

\item STREAMS better exploits parallelisms on SMP better than other approaches.

STREAMS showed significant improvements over Sockets on SMP.  In fact, STREAMS
outperformed Sockets to a larger degree on SMP tests than on UP tests.

\item STREAMS networking is not slow.

If STREAMS networking is slow, the tests prove that Sockets is slower.

\item STREAMS networking is neither unnecessarily complex nor cumbersome.

\end{itemize}

\section[Related Work]{Related Work}

\section[Future Work]{Future Work}

UNIX domain sockets are the advocated primary interprocess communications
mechanism in the BSD system.  Linux also implements UNIX domain sockets.

XTI has an equivalent to the UNIX domain socket.  This consists of
connectionless, connection oriented, and connection oriented with orderly
release transport providers.

The \texttt{netperf} program has the ability to test UNIX domain sockets, but
does not currently have the ability to test the XTI equivalents.

BSD claims that in 4.4BSD pipes were implemented using sockets (UNIX domain
sockets) instead of using the file system as they were in 4.2BSD.  One of the
reasons cited for implementing pipes on Sockets and UNIX domain sockets using
the networking subsystems was performance.  Another paper released by the
\textsl{OpenSS7 Project}\footnote{\textit{http://www.openss7.org/}} shows that
experimental results on Linux file-system based pipes (using the SVR3 or
4.2BSD approaches) perform poorly when compared to STREAMS-based pipes.
Because Linux uses a similar approach to file-system based pipes in
implementation of UNIX domain sockets, it can be expected that UNIX domain
sockets under Linux will also perform poorly when compared to loop-back
transport providers under STREAMS.

\FloatBarrier
\addcontentsline{toc}{section}{References}
\bibliography{testresults}

\clearpage
\begin{appendix}

\section[Netperf Benchmark Script]{Netperf Benchmark Script}
\label{section:script}

Following is a listing of the \texttt{netperf\_benchmark} script used to
generate raw data points for analysis:

\small
\begin{verbatim}
#!/bin/bash
set -x
(
  sudo killall netserver
  sudo netserver >/dev/null </dev/null 2>/dev/null &
  sleep 3
  netperf_udp_range -x /dev/udp2 \
    --testtime=10 --bufsizes=131071 --end=16384 ${1+"$@"}
  netperf_udp_range \
    --testtime=10 --bufsizes=131071 --end=16384 ${1+"$@"}
  netperf_udp_range -x /dev/udp \
    --testtime=10 --bufsizes=131071 --end=16384 ${1+"$@"}
  sudo killall netserver
) 2>&1 | tee `hostname`.`date -uIminutes`.log
\end{verbatim}

\section[Raw Data]{Raw Data}

Following are the raw data points captured using the
\texttt{netperf\_benchmark} script:

\textit{Table \ref{table:fc6data} (p.  \pageref{table:fc6data})} lists the raw
data from the \texttt{netperf} program that was used in preparing graphs for
Fedora Core 6 (i386) on Porky.

\textit{Table \ref{table:centosdata} (p. \pageref{table:centosdata})} lists
the raw data from the \texttt{netperf} program that was used in preparing
graphs for CentOS 4 on Porky.

\textit{Table \ref{table:susedata} (p. \pageref{table:susedata})} lists the
raw data from the \texttt{netperf} program that was used in preparing graphs
for SuSE OSS 10 on Porky.

\textit{Table \ref{table:ubuntudata} (p. \pageref{table:ubuntudata})} lists
the raw data from the \texttt{netperf} program that was used in preparing
graphs for Ubuntu 6.06 on Porky.

\textit{Table \ref{table:rh7data} (p. \pageref{table:rh7data})} lists the raw
data from the \texttt{netperf} program that was used in preparing graphs for
RedHat 7.2 on Pumbah.

\textit{Table \ref{table:smpdata} (p. \pageref{table:smpdata})} lists the raw
data from the \texttt{netperf} program that was used in preparing graphs for
Fedora Core 6 (x86\_64) HT on Daisy.

\textit{Table \ref{table:nbdata} (p. \pageref{table:nbdata})} lists the raw
data from the \texttt{netperf} program that was used in preparing graphs for
SuSE 10 OSS on Mspiggy.

\begin{table}[hbp]
\normalsize
\begin{center}
\setlength{\tabcolsep}{0.3em}
\setlength{\arraycolsep}{0.3em}
\begin{tabular}{rrrrrrr}\\
Message & \multicolumn{2}{c}{XTIoS} & \multicolumn{2}{c}{XTI} & \multicolumn{2}{c}{Sockets}\\
Size & Tx & Rx & Tx & Rx & Tx & Rx\\
\hline
\hline
1 & 714927 & 714928 & 947084 & 947085 & 740775 & 728170\\
2 & 717371 & 717372 & 934792 & 934793 & 745202 & 732710\\
4 & 713453 & 713454 & 938505 & 938506 & 750541 & 730419\\
8 & 704000 & 704001 & 935024 & 935025 & 745011 & 724798\\
16 & 697051 & 697052 & 930898 & 930899 & 746454 & 731250\\
32 & 688597 & 688598 & 931763 & 931764 & 748286 & 731657\\
64 & 686784 & 686785 & 939694 & 939695 & 740980 & 722478\\
128 & 674447 & 674448 & 930575 & 930576 & 742196 & 723733\\
256 & 657051 & 657052 & 907451 & 907452 & 740007 & 717115\\
512 & 651677 & 651678 & 902984 & 902985 & 718341 & 708200\\
1024 & 619363 & 619364 & 868516 & 868517 & 712384 & 693917\\
2048 & 559866 & 559867 & 793259 & 793260 & 684433 & 674277\\
4096 & 459220 & 459221 & 706605 & 706606 & 629194 & 612532\\
8192 & 367311 & 367312 & 627682 & 627683 & 554245 & 541436\\
16384 & 249573 & 249574 & 469472 & 469473 & 446906 & 437599\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption[FC6 on Porky Raw Data]{FC6 on Porky Raw Data}
\label{table:fc6data}
\end{table}

\begin{table}[hbp]
\normalsize
\begin{center}
\setlength{\tabcolsep}{0.3em}
\setlength{\arraycolsep}{0.3em}
\begin{tabular}{rrrrrrr}\\
Message & \multicolumn{2}{c}{XTIoS} & \multicolumn{2}{c}{XTI} & \multicolumn{2}{c}{Sockets}\\
Size & Tx & Rx & Tx & Rx & Tx & Rx\\
\hline
\hline
1 & 849555 & 849556 & 1167336 & 1167337 & 861219 & 860982\\
2 & 845106 & 845107 & 1171086 & 1171087 & 860981 & 860257\\
4 & 848669 & 848670 & 1171198 & 1171199 & 863027 & 862307\\
8 & 828520 & 828521 & 1158247 & 1158248 & 859350 & 858899\\
16 & 835946 & 835947 & 1163405 & 1163406 & 856881 & 856418\\
32 & 837624 & 837625 & 1145328 & 1145329 & 861550 & 861133\\
64 & 824114 & 824115 & 1156624 & 1156625 & 850320 & 849599\\
128 & 811344 & 811345 & 1160676 & 1160677 & 847531 & 846980\\
256 & 813958 & 813959 & 1154616 & 1154617 & 842601 & 841396\\
512 & 804584 & 804585 & 1164623 & 1164624 & 833461 & 832452\\
1024 & 767812 & 767813 & 1118676 & 1118677 & 808018 & 806991\\
2048 & 693760 & 693761 & 1050507 & 1050508 & 766594 & 765236\\
4096 & 561885 & 561886 & 920261 & 920262 & 682312 & 681197\\
8192 & 437609 & 437610 & 678034 & 678035 & 598846 & 597855\\
16384 & 268808 & 268809 & 590358 & 590359 & 478197 & 477303\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption[CentOS 4 on Porky Raw Data]{CentOS 4 on Porky Raw Data}
\label{table:centosdata}
\end{table}

\begin{table}[hbp]
\normalsize
\begin{center}
\setlength{\tabcolsep}{0.3em}
\setlength{\arraycolsep}{0.3em}
\begin{tabular}{rrrrrrr}\\
Message & \multicolumn{2}{c}{XTIoS} & \multicolumn{2}{c}{XTI} & \multicolumn{2}{c}{Sockets}\\
Size & Tx & Rx & Tx & Rx & Tx & Rx\\
\hline
\hline
1 & 573781 & 573782 & 713504 & 713505 & 594660 & 594467\\
2 & 567733 & 567734 & 720039 & 720040 & 587883 & 587791\\
4 & 569997 & 569998 & 729645 & 729646 & 589438 & 589229\\
8 & 567197 & 567198 & 734516 & 734517 & 589559 & 589416\\
16 & 568657 & 568658 & 686428 & 686429 & 593745 & 593600\\
32 & 571096 & 571097 & 689929 & 689930 & 594827 & 594671\\
64 & 570663 & 570664 & 705258 & 705259 & 593679 & 593128\\
128 & 567062 & 567063 & 706918 & 706919 & 592829 & 592829\\
256 & 568372 & 568373 & 716627 & 716628 & 585737 & 585338\\
512 & 565382 & 565383 & 675129 & 675130 & 581023 & 580381\\
1024 & 546251 & 546252 & 633631 & 633632 & 576955 & 576220\\
2048 & 510822 & 510823 & 627276 & 627277 & 556534 & 555734\\
4096 & 437420 & 437421 & 577926 & 577927 & 518700 & 517611\\
8192 & 353468 & 353469 & 528576 & 528577 & 458838 & 458081\\
16384 & 258953 & 258954 & 455257 & 455258 & 378575 & 377998\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption[SuSE OSS 10 on Porky Raw Data]{SuSE OSS 10 on Porky Raw Data}
\label{table:susedata}
\end{table}

\begin{table}[hbp]
\normalsize
\begin{center}
\setlength{\tabcolsep}{0.3em}
\setlength{\arraycolsep}{0.3em}
\begin{tabular}{rrrrrrr}\\
Message & \multicolumn{2}{c}{XTIoS} & \multicolumn{2}{c}{XTI} & \multicolumn{2}{c}{Sockets}\\
Size & Tx & Rx & Tx & Rx & Tx & Rx\\
\hline
\hline
1 & 529545 & 529546 & 662574 & 662575 & 615243 & 615243\\
2 & 529833 & 529834 & 662749 & 662750 & 615219 & 615219\\
4 & 529409 & 529410 & 662601 & 662602 & 614769 & 614769\\
8 & 526374 & 526375 & 652110 & 652111 & 614941 & 614941\\
16 & 527462 & 527463 & 654046 & 654047 & 614494 & 614494\\
32 & 525083 & 525084 & 649961 & 649962 & 614532 & 614532\\
64 & 524388 & 524389 & 648902 & 648903 & 613586 & 613586\\
128 & 521954 & 521955 & 650092 & 650093 & 612867 & 612867\\
256 & 508588 & 508589 & 644845 & 644846 & 598102 & 598102\\
512 & 505348 & 505349 & 642097 & 642098 & 595758 & 595758\\
1024 & 481918 & 481919 & 623680 & 623681 & 590474 & 590474\\
2048 & 451341 & 451342 & 600956 & 600957 & 568011 & 568011\\
4096 & 390587 & 390588 & 552289 & 552290 & 529874 & 529874\\
8192 & 304485 & 304486 & 499277 & 499278 & 466069 & 466069\\
16384 & 232667 & 232668 & 405488 & 405489 & 391741 & 391741\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption[Ubuntu 6.06 on Porky Raw Data]{Ubuntu 6.06 on Porky Raw Data}
\label{table:ubuntudata}
\end{table}

\begin{table}[hbp]
\normalsize
\begin{center}
\setlength{\tabcolsep}{0.3em}
\setlength{\arraycolsep}{0.3em}
\begin{tabular}{rrrrrrr}\\
Message & \multicolumn{2}{c}{XTIoS} & \multicolumn{2}{c}{XTI} & \multicolumn{2}{c}{Sockets}\\
Size & Tx & Rx & Tx & Rx & Tx & Rx\\
\hline
\hline
1 & 1133043 & 1133044 & 1560516 & 1560517 & 1422429 & 1422429\\
2 & 1136533 & 1136534 & 1562461 & 1562462 & 1418493 & 1418493\\
4 & 1136695 & 1136696 & 1578993 & 1578994 & 1415739 & 1415129\\
8 & 1142312 & 1142313 & 1578110 & 1578111 & 1415738 & 1415129\\
16 & 1139623 & 1139624 & 1571645 & 1571646 & 1412013 & 1411527\\
32 & 1140659 & 1140660 & 1573956 & 1573957 & 1418429 & 1418429\\
64 & 1136007 & 1136008 & 1574064 & 1574065 & 1406332 & 1406332\\
128 & 1106231 & 1106232 & 1541064 & 1541065 & 1370828 & 1370828\\
256 & 1073676 & 1073677 & 1535408 & 1535409 & 1358240 & 1357444\\
512 & 1026932 & 1026933 & 1517692 & 1517693 & 1299434 & 1299434\\
1024 & 941481 & 941482 & 1455261 & 1455262 & 1211158 & 1211158\\
2048 & 793802 & 793803 & 1351690 & 1351691 & 1073543 & 1073543\\
4096 & 610252 & 610253 & 1216734 & 1216735 & 872281 & 872281\\
8192 & 416164 & 416165 & 1033488 & 1033489 & 644953 & 644953\\
16384 & 248762 & 248763 & 780198 & 779901 & 419478 & 419478\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption[RedHat 7.2 on Pumbah Raw Data]{RedHat 7.2 on Pumbah Raw Data}
\label{table:rh7data}
\end{table}

\begin{table}[hbp]
\normalsize
\begin{center}
\setlength{\tabcolsep}{0.3em}
\setlength{\arraycolsep}{0.3em}
\begin{tabular}{rrrrrrr}\\
Message & \multicolumn{2}{c}{XTIoS} & \multicolumn{2}{c}{XTI} & \multicolumn{2}{c}{Sockets}\\
Size & Tx & Rx & Tx & Rx & Tx & Rx\\
\hline
\hline
1 & 553383 & 553384 & 1009820 & 1009820 & 731713 & 731713\\
2 & 550020 & 550021 & 1005658 & 1005659 & 726596 & 726596\\
4 & 549600 & 549601 & 993347 & 993348 & 733634 & 733634\\
8 & 549073 & 549074 & 1000195 & 1000196 & 724320 & 724320\\
16 & 549514 & 549515 & 1000525 & 1000526 & 725440 & 725440\\
32 & 548447 & 548447 & 1007185 & 1007186 & 728707 & 728707\\
64 & 545329 & 545330 & 994739 & 994740 & 720612 & 720612\\
128 & 540519 & 540520 & 999002 & 999003 & 722801 & 722801\\
256 & 521171 & 521172 & 994474 & 994475 & 723606 & 723606\\
512 & 508589 & 508590 & 982028 & 982029 & 709207 & 709207\\
1024 & 483899 & 483900 & 951564 & 951565 & 707136 & 707136\\
2048 & 446004 & 446005 & 897395 & 897396 & 688775 & 688775\\
4096 & 387509 & 387510 & 795327 & 795328 & 650128 & 650128\\
8192 & 302141 & 302142 & 677573 & 677573 & 605011 & 605011\\
16384 & 211149 & 211150 & 505129 & 505130 & 503729 & 503729\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption[Fedora Core 6 (x86\_64) HT on Daisy Raw Data]{Fedora Core 6 (x86\_64) HT on Daisy Raw Data}
\label{table:smpdata}
\end{table}

\begin{table}[hbp]
\normalsize
\begin{center}
\setlength{\tabcolsep}{0.3em}
\setlength{\arraycolsep}{0.3em}
\begin{tabular}{rrrrrrr}\\
Message & \multicolumn{2}{c}{XTIoS} & \multicolumn{2}{c}{XTI} & \multicolumn{2}{c}{Sockets}\\
Size & Tx & Rx & Tx & Rx & Tx & Rx\\
\hline
\hline
1 & 479564 & 479565 & 591461 & 591462 & 482975 & 481652\\
2 & 480678 & 480679 & 592805 & 592806 & 481606 & 480276\\
4 & 478366 & 478367 & 593255 & 593256 & 480746 & 479680\\
8 & 473615 & 473616 & 589930 & 589931 & 479021 & 477301\\
16 & 471973 & 471974 & 585814 & 585815 & 478449 & 476241\\
32 & 474980 & 474981 & 585272 & 585273 & 480508 & 478812\\
64 & 466618 & 466619 & 587244 & 587245 & 474745 & 472577\\
128 & 465623 & 465624 & 582449 & 582450 & 472031 & 470381\\
256 & 458158 & 458159 & 587534 & 587534 & 466018 & 463747\\
512 & 446356 & 446357 & 586409 & 586410 & 450769 & 448312\\
1024 & 421072 & 421073 & 567213 & 567214 & 435038 & 433157\\
2048 & 368990 & 368991 & 543818 & 543819 & 397745 & 395329\\
4096 & 290402 & 290403 & 500380 & 500381 & 344058 & 341942\\
8192 & 218918 & 218919 & 438956 & 438957 & 265907 & 264098\\
16384 & 137005 & 137006 & 348956 & 348957 & 192224 & 191737\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption[SuSE 10 OSS on Mspiggy Raw Data]{SuSE 10 OSS on Mspiggy Raw Data}
\label{table:nbdata}
\end{table}

\end{appendix}

\end{document}
